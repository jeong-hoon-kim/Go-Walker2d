import gymnasium as gym
from gymnasium import Wrapper
from stable_baselines3 import PPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.utils import set_random_seed
from stable_baselines3.common.callbacks import BaseCallback
import numpy as np
import os
import torch

# --- 1. ì»¤ìŠ¤í…€ ë³´ìƒ ë˜í¼ í´ë˜ìŠ¤ ---
import gymnasium as gym
from gymnasium import Wrapper
import numpy as np

class GaitRewardWrapper(Wrapper):
    def __init__(self, env):
        super().__init__(env)
        self.time_step = 0
        
        # --- ê±¸ìŒê±¸ì´ íŒ¨í„´ì„ ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ---
        # ì´ ê°’ë“¤ì„ ì¡°ì •í•˜ë©° ìµœì ì˜ ê±¸ìŒê±¸ì´ë¥¼ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.
        self.gait_frequency = 2.5  # ê±¸ìŒê±¸ì´ ì†ë„ (Hz)
        self.hip_amplitude = 0.5   # í—ˆë²…ì§€ ê´€ì ˆì˜ ì›€ì§ì„ í­ (radian)
        self.knee_amplitude = 0.5  # ë¬´ë¦ ê´€ì ˆì˜ ì›€ì§ì„ í­ (radian)
        self.gait_reward_weight = 0.2 # íŒ¨í„´ ë³´ìƒì˜ ê°€ì¤‘ì¹˜

    def reset(self, **kwargs):
        self.time_step = 0
        return self.env.reset(**kwargs)

    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        self.time_step += self.env.unwrapped.dt # ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„ ì—…ë°ì´íŠ¸

        # --- ğŸ† ê±¸ìŒê±¸ì´ íŒ¨í„´ ë³´ìƒ (Gait Pattern Reward) ---
        
        # 1. ì´ìƒì ì¸ ëª©í‘œ ê°ë„ë¥¼ ì‚¬ì¸íŒŒë¡œ ê³„ì‚°
        # í˜„ì¬ ì‹œê°„(t)ì„ ê¸°ë°˜ìœ¼ë¡œ ê° ê´€ì ˆì´ ê°€ì ¸ì•¼ í•  ì´ìƒì ì¸ ê°ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        phase = 2 * np.pi * self.gait_frequency * self.time_step
        
        # ì˜¤ë¥¸ìª½ ë‹¤ë¦¬ì˜ ëª©í‘œ ê°ë„
        target_right_hip_angle = self.hip_amplitude * np.sin(phase)
        target_right_knee_angle = self.knee_amplitude * np.sin(phase + np.pi / 2) # ë¬´ë¦ì€ ìœ„ìƒì´ ì•½ê°„ ë‹¤ë¦„
        
        # ì™¼ìª½ ë‹¤ë¦¬ëŠ” ì˜¤ë¥¸ìª½ê³¼ 180ë„(pi) ë°˜ëŒ€ ìœ„ìƒ
        target_left_hip_angle = self.hip_amplitude * np.sin(phase + np.pi)
        target_left_knee_angle = self.knee_amplitude * np.sin(phase + np.pi + np.pi / 2)

        # 2. ì‹¤ì œ ê´€ì ˆ ê°ë„ì™€ ëª©í‘œ ê°ë„ì˜ ì°¨ì´ ê³„ì‚°
        # obs ë²¡í„°ì—ì„œ ì‹¤ì œ ê´€ì ˆ ê°ë„ ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        actual_right_hip_angle = obs[2]
        actual_right_knee_angle = obs[3] # ì¢…ì•„ë¦¬ ê´€ì ˆì´ ë¬´ë¦ ì—­í• 
        actual_left_hip_angle = obs[5]
        actual_left_knee_angle = obs[6]

        # 3. ì˜¤ì°¨ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í˜ë„í‹° ê³„ì‚° (ì˜¤ì°¨ê°€ ì‘ì„ìˆ˜ë¡ í˜ë„í‹°ê°€ ì ìŒ)
        hip_error = (actual_right_hip_angle - target_right_hip_angle)**2 + \
                    (actual_left_hip_angle - target_left_hip_angle)**2
        knee_error = (actual_right_knee_angle - target_right_knee_angle)**2 + \
                     (actual_left_knee_angle - target_left_knee_angle)**2
                     
        # ì˜¤ì°¨ê°€ í´ìˆ˜ë¡ í° í˜ë„í‹°ë¥¼ ë¶€ì—¬ (ë³´ìƒ = -ê°€ì¤‘ì¹˜ * ì˜¤ì°¨)
        gait_penalty = -self.gait_reward_weight * (hip_error + knee_error)

        # 4. ìµœì¢… ë³´ìƒì— í•©ì‚°
        new_reward = reward + gait_penalty
        
        return obs, new_reward, terminated, truncated, info

# --- 2. ì»¤ìŠ¤í…€ í‰ê°€ ì½œë°± í´ë˜ìŠ¤ ì •ì˜ ---
class AdvancedEvalCallback(BaseCallback):
    def __init__(self, eval_env, save_path, eval_freq=20000, n_eval_episodes=5, verbose=1):
        super(AdvancedEvalCallback, self).__init__(verbose)
        self.eval_env = eval_env
        self.save_path = save_path
        self.eval_freq = eval_freq
        self.n_eval_episodes = n_eval_episodes
        
        self.best_mean_distance = -np.inf
        self.best_mean_stability = np.inf

    def _on_step(self) -> bool:
        if self.n_calls % self.eval_freq == 0:
            episode_distances, episode_stabilities = [], []
            for _ in range(self.n_eval_episodes):
                obs, info = self.eval_env.reset()
                done = False
                torso_angles = []
                final_distance = 0
                while not done:
                    action, _ = self.model.predict(obs, deterministic=True)
                    obs, reward, terminated, truncated, info = self.eval_env.step(action)
                    done = terminated or truncated
                    torso_angles.append(obs[1])
                    if done: final_distance = info.get('x_position', 0)
                
                episode_distances.append(final_distance)
                episode_stabilities.append(np.std(torso_angles))

            mean_distance = np.mean(episode_distances)
            mean_stability = np.mean(episode_stabilities)
            
            self.logger.record("eval/mean_distance", mean_distance)
            self.logger.record("eval/mean_stability", mean_stability)

            if self.verbose > 0:
                print(f"--- Timestep {self.num_timesteps}: Custom Eval ---")
                print(f"Avg Distance: {mean_distance:.2f} m, Avg Stability: {mean_stability:.4f}")

            # ìµœê³  ì´ë™ ê±°ë¦¬ ëª¨ë¸ ì €ì¥
            if mean_distance > self.best_mean_distance:
                self.best_mean_distance = mean_distance
                self.model.save(os.path.join(self.save_path, "best_distance_model.zip"))
                if self.verbose > 0: print(f"  >> New best distance model saved: {mean_distance:.2f} m")

            # ìµœê³  ì•ˆì •ì„± ëª¨ë¸ ì €ì¥
            if mean_stability < self.best_mean_stability:
                self.best_mean_stability = mean_stability
                self.model.save(os.path.join(self.save_path, "best_stability_model.zip"))
                if self.verbose > 0: print(f"  >> New best stability model saved: {mean_stability:.4f}")
            print("---------------------------------")
        
        return True

# --- 3. ë©”ì¸ í›ˆë ¨ ì½”ë“œ ---
if __name__ == "__main__":
    # --- ì„¤ì • ---
    MODEL_NAME = "ppo_walker2d_custom_reward_v1"
    SAVE_PATH = f"results/{MODEL_NAME}/"
    LOG_PATH = "tensorboard_logs/"
    TOTAL_TIMESTEPS = 3000000
    SEED = 42

    set_random_seed(SEED)
    os.makedirs(SAVE_PATH, exist_ok=True)
    os.makedirs(LOG_PATH, exist_ok=True)

    # í›ˆë ¨ìš© í™˜ê²½
    train_env = gym.make("Walker2d-v5")
    train_env = GaitRewardWrapper(train_env)
    train_env = Monitor(train_env, SAVE_PATH)

    # í‰ê°€ìš© í™˜ê²½
    eval_env = gym.make("Walker2d-v5")

    # ì‚¬ìš© ê°€ëŠ¥í•œ ì¥ì¹˜ í™•ì¸ (GPU ìš°ì„ )
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    # ì½œë°± ì„¤ì •
    callback = AdvancedEvalCallback(eval_env, save_path=SAVE_PATH)

    # ëª¨ë¸ ìƒì„±í•˜ê¸°
    model = PPO(
        "MlpPolicy", 
        train_env, 
        verbose=1, 
        seed=SEED, 
        device=device,
        tensorboard_log=LOG_PATH 
    )

    # ëª¨ë¸ í•™ìŠµì‹œí‚¤ê¸°
    model.learn(
        total_timesteps=TOTAL_TIMESTEPS,
        callback=callback,
        tb_log_name=MODEL_NAME
    )

    # ìµœì¢… ëª¨ë¸ ì €ì¥í•˜ê¸°
    model.save(f"{SAVE_PATH}{MODEL_NAME}_final.zip")
    print("ìµœì¢… ëª¨ë¸ ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    train_env.close()
    eval_env.close()