import torch
import numpy as np
import gymnasium as gym
from gymnasium import Wrapper
from gymnasium.wrappers import RecordVideo
from gymnasium.envs.mujoco import walker2d_v5
from gymnasium.wrappers import TimeLimit
from stable_baselines3 import SAC
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.utils import set_random_seed
from stable_baselines3.common.callbacks import BaseCallback
import os
import datetime
import utils


## --- ì»¤ìŠ¤í…€ ë¦¬ì›Œë“œ ë˜í¼ ì •ì˜ ---
class PhasedGaitWrapper(Wrapper):
    def __init__(self, env):
        super().__init__(env)
        
        # --- ğŸ† í•˜ì´í¼íŒŒë¼ë¯¸í„° ---
        # ê¸°ì¡´ íŒŒë¼ë¯¸í„°ë“¤
        self.target_velocity = 1.75
        self.velocity_tolerance = 0.5
        self.velocity_reward_weight = 2.0
        self.stability_weight = 0.3
        self.flight_penalty_weight = 1.0
        
        # --- ì¶”ê°€ëœ ë¶€ë¶„: ë¶„ë©´ ë³´ìƒ ê´€ë ¨ ---
        self.phase_reward_weight = 0.05 # ë¶„ë©´ ë³´ìƒ ê°€ì¤‘ì¹˜ (ì›ë˜ 0.005ì—ì„œ 0.05ë¡œ ìƒí–¥ ì¡°ì • ê¶Œì¥)
        self.current_phase = 0 # í˜„ì¬ ë³´í–‰ ë‹¨ê³„ (0ì€ ì´ˆê¸°/ì •ì˜ë˜ì§€ ì•Šì€ ìƒíƒœ)
        self.phase_definition_threshold = 0.1 # ê°ì†ë„ê°€ 0ì— ê°€ê¹ë‹¤ê³  íŒë‹¨í•  ì„ê³„ê°’

        # ë°œê³¼ ë°”ë‹¥ì˜ ID
        # Mujoco í™˜ê²½ì˜ ëª¨ë¸ êµ¬ì¡°ì— ë”°ë¼ ID ê°€ì ¸ì˜¤ëŠ” ë¶€ë¶„ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
        self.left_foot_geom_id = self.env.unwrapped.model.geom('foot_left_geom').id
        self.right_foot_geom_id = self.env.unwrapped.model.geom('foot_geom').id
        self.floor_geom_id = self.env.unwrapped.model.geom('floor').id
        
    def _check_foot_contact(self):
        # ... (ê¸°ì¡´ê³¼ ë™ì¼) ...
        left_contact, right_contact = False, False
        for contact in self.env.unwrapped.data.contact:
            geom_pair = {contact.geom1, contact.geom2}
            if self.left_foot_geom_id in geom_pair and self.floor_geom_id in geom_pair: left_contact = True
            if self.right_foot_geom_id in geom_pair and self.floor_geom_id in geom_pair: right_contact = True
        return left_contact, right_contact

    # --- ğŸ† ê°œì„ ëœ ë¶€ë¶„: 6ë‹¨ê³„ ë³´í–‰ ë‹¨ê³„ íŒë‹¨ í•¨ìˆ˜ ---
    def _get_current_phase(self, obs, left_contact, right_contact):
        # í—ˆë²…ì§€ ê´€ì ˆ ê°ì†ë„ (Thigh Angular Velocity)
        # obs[11]: ì˜¤ë¥¸ìª½ í—ˆë²…ì§€ ê°ì†ë„, obs[14]: ì™¼ìª½ í—ˆë²…ì§€ ê°ì†ë„
        # ê±·ëŠ” ë°©í–¥ìœ¼ë¡œì˜ íšŒì „ì„ ì–‘ìˆ˜, ë’¤ë¡œ ë¯¸ëŠ” ë™ì‘ì„ ìŒìˆ˜ë¡œ ê°€ì •í•©ë‹ˆë‹¤.
        right_thigh_vel = obs[11]
        left_thigh_vel = obs[14]

        threshold = self.phase_definition_threshold
        
        # --- 6ë‹¨ê³„ ë³´í–‰ ì£¼ê¸° ì •ì˜ (ì˜¤ë¥¸ë°œ ì£¼ê¸°: 1->2->3, ì™¼ë°œ ì£¼ê¸°: 4->5->6) ---
        
        # 1. Initial Double Support (IDS, ì˜¤ë¥¸ë°œ ì°©ì§€ ì§í›„)
        # L/R ëª¨ë‘ ì ‘ì´‰ & ì˜¤ë¥¸ë°œ ìŠ¤ìœ™ ë/ì •ì§€ & ì™¼ë°œì€ ì•„ì§ ë’¤ë¡œ ë°€ê³  ìˆìŒ
        if left_contact and right_contact and \
           (right_thigh_vel < threshold and right_thigh_vel > -threshold) and left_thigh_vel < -threshold:
            return 1
            
        # 2. Right Single Support (RSS, ì˜¤ë¥¸ë°œ ë‹¨ë… ì§€ì§€ - ì™¼ë°œ ìŠ¤ìœ™ ì‹œì‘)
        # ì™¼ë°œ ê³µì¤‘ & ì˜¤ë¥¸ë°œ ì§€ì§€ & ì™¼ë°œì´ ì•ìœ¼ë¡œ ìŠ¤ìœ™
        elif not left_contact and right_contact and \
             right_thigh_vel <= threshold and left_thigh_vel > threshold:
            return 2

        # 3. Terminal Double Support (TDS, ì™¼ë°œ ì°©ì§€ ì§ì „)
        # L/R ëª¨ë‘ ì ‘ì´‰ & ì˜¤ë¥¸ë°œì´ ë’¤ë¡œ ë°€ì–´ë‚´ê³  & ì™¼ë°œ ìŠ¤ìœ™ì´ ëë‚˜ ì°©ì§€ ì¤€ë¹„ ì¤‘
        elif left_contact and right_contact and \
             right_thigh_vel < -threshold and (left_thigh_vel < threshold and left_thigh_vel > -threshold):
            return 3 
            
        # --- ëŒ€ì¹­ ë‹¨ê³„ (ì™¼ë°œ ì£¼ê¸°) ---
        
        # 4. Initial Double Support (IDS, ì™¼ë°œ ì°©ì§€ ì§í›„)
        # L/R ëª¨ë‘ ì ‘ì´‰ & ì™¼ë°œ ìŠ¤ìœ™ ë/ì •ì§€ & ì˜¤ë¥¸ë°œì€ ì•„ì§ ë’¤ë¡œ ë°€ê³  ìˆìŒ
        elif left_contact and right_contact and \
             (left_thigh_vel < threshold and left_thigh_vel > -threshold) and right_thigh_vel < -threshold:
            return 4
            
        # 5. Left Single Support (LSS, ì™¼ë°œ ë‹¨ë… ì§€ì§€ - ì˜¤ë¥¸ë°œ ìŠ¤ìœ™ ì‹œì‘)
        # ì˜¤ë¥¸ë°œ ê³µì¤‘ & ì™¼ë°œ ì§€ì§€ & ì˜¤ë¥¸ë°œì´ ì•ìœ¼ë¡œ ìŠ¤ìœ™
        elif left_contact and not right_contact and \
             left_thigh_vel <= threshold and right_thigh_vel > threshold:
            return 5

        # 6. Terminal Double Support (TDS, ì˜¤ë¥¸ë°œ ì°©ì§€ ì§ì „)
        # L/R ëª¨ë‘ ì ‘ì´‰ & ì™¼ë°œì´ ë’¤ë¡œ ë°€ì–´ë‚´ê³  & ì˜¤ë¥¸ë°œ ìŠ¤ìœ™ì´ ëë‚˜ ì°©ì§€ ì¤€ë¹„ ì¤‘
        elif left_contact and right_contact and \
             left_thigh_vel < -threshold and (right_thigh_vel < threshold and right_thigh_vel > -threshold):
            return 6
            
        else: # ë‘ ë°œ ê³µì¤‘ ë˜ëŠ” ì •ì˜ëœ 6ë‹¨ê³„ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ì• ë§¤í•œ ìƒíƒœ
            return 0

    def reset(self, **kwargs):
        self.current_phase = 0 # ë‹¨ê³„ ì´ˆê¸°í™”
        obs, info = self.env.reset(**kwargs)
        return obs, info

    def step(self, action):
        obs, original_reward, terminated, truncated, info = self.env.step(action)

        healthy_reward = info.get('reward_survive', 1.0)
        ctrl_cost = info.get('reward_ctrl', 0)
        # Stability: ë¡¤ë§(obs[1])ê³¼ í”¼ì¹­ ê°ì†ë„(obs[10]) í˜ë„í‹°
        stability_penalty = -self.stability_weight * (np.abs(obs[1]) + 0.1 * np.abs(obs[10]))
        
        current_velocity = obs[8] # ì „ë°© ì†ë„
        velocity_bonus = self.velocity_reward_weight * \
                          np.exp(-np.square(current_velocity - self.target_velocity) / (2 * np.square(self.velocity_tolerance)))
        
        left_foot_on_ground, right_foot_on_ground = self._check_foot_contact()
        flight_penalty = 0
        if not left_foot_on_ground and not right_foot_on_ground:
            flight_penalty = -self.flight_penalty_weight

        # --- ğŸ† ê°œì„ ëœ ë¶€ë¶„: 6ë‹¨ê³„ ë¶„ë©´ ë³´ìƒ (Phase Reward) ê³„ì‚° ---
        previous_phase = self.current_phase
        current_phase = self._get_current_phase(obs, left_foot_on_ground, right_foot_on_ground)
        
        phase_reward = 0
        # 6ë‹¨ê³„ ìˆœí™˜: 1->2, 2->3, 3->4, 4->5, 5->6, 6->1
        expected_next_phase = (previous_phase % 6) + 1 # 1ë¶€í„° 6ê¹Œì§€ ìˆœí™˜ (6 ë‹¤ìŒì€ 1)

        if previous_phase != 0 and current_phase == expected_next_phase: 
            # ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ì „ì´: +1 ë³´ìƒ
            phase_reward = 1
        elif current_phase == previous_phase or current_phase == 0 or previous_phase == 0: 
            # ë‹¨ê³„ ìœ ì§€, ìœ íš¨í•˜ì§€ ì•Šì€ ë‹¨ê³„(0), ë˜ëŠ” ì´ˆê¸° ìƒíƒœ: ë³´ìƒ/í˜ë„í‹° ì—†ìŒ
            phase_reward = 0
        else: 
            # ì˜ëª»ëœ ìˆœì„œë¡œ ì „ì´ (ì˜ˆ: 1->3 ë˜ëŠ” 1->6): -1 í˜ë„í‹°
            phase_reward = -1 
            
        # í˜„ì¬ ë‹¨ê³„ë¥¼ ë‹¤ìŒ ìŠ¤í…ì„ ìœ„í•´ ì €ì¥
        if current_phase != 0: # ìœ íš¨í•œ ë‹¨ê³„ì¼ ë•Œë§Œ ì—…ë°ì´íŠ¸
            self.current_phase = current_phase

        # --- ìµœì¢… ë³´ìƒ ê³„ì‚° ---
        new_reward = (
            velocity_bonus
            + healthy_reward
            + ctrl_cost
            + stability_penalty
            + flight_penalty
            + (self.phase_reward_weight * phase_reward) # <-- ë¶„ë©´ ë³´ìƒ ì¶”ê°€
        )
        
        # infoì— í˜„ì¬ ë‹¨ê³„ ì¶”ê°€ (ë””ë²„ê¹…ìš©)
        info['current_phase'] = self.current_phase
        info['phase_reward'] = self.phase_reward_weight * phase_reward
        
        return obs, new_reward, terminated, truncated, info

## --- ì»¤ìŠ¤í…€ í‰ê°€ ì½œë°± í´ë˜ìŠ¤ ì •ì˜ ---
class AdvancedEvalCallback(BaseCallback):
    def __init__(self, eval_env, save_path, eval_freq, n_eval_episodes, verbose):
        super(AdvancedEvalCallback, self).__init__(verbose)
        self.eval_env = eval_env
        self.save_path = save_path
        self.eval_freq = eval_freq
        self.n_eval_episodes = n_eval_episodes
        
        # ê° ì§€í‘œë³„ ìµœê³  ê¸°ë¡ì„ ì €ì¥í•  ë³€ìˆ˜
        self.best_mean_distance = -np.inf
        self.best_mean_stability = np.inf
        self.best_reward = -np.inf

    def _on_step(self) -> bool:
        if self.n_calls % self.eval_freq == 0:
            episode_distances, episode_stabilities, episode_reward = [], [], []
            for _ in range(self.n_eval_episodes):
                obs, info = self.eval_env.reset()
                done = False
                torso_angles = []
                final_distance = 0
                episode_total_reward = 0.0 # ëˆ„ì  ë³´ìƒ ì´ˆê¸°í™”

                while not done:
                    action, _ = self.model.predict(obs, deterministic=True)
                    obs, reward, terminated, truncated, info = self.eval_env.step(action)
                    done = terminated or truncated
                    
                    episode_total_reward += reward # ëˆ„ì  ë³´ìƒ ì—…ë°ì´íŠ¸

                    # Walker2d-v5ì˜ obs[1]ëŠ” ëª¸í†µ ê°ë„(torso angle)ì…ë‹ˆë‹¤. (v3, v4ì™€ ë‹¤ë¦„)
                    torso_angles.append(obs[1]) 
                    if done: 
                        final_distance = info.get('x_position', 0)
                
                episode_distances.append(final_distance)
                episode_stabilities.append(np.std(torso_angles))
                episode_reward.append(episode_total_reward)

            mean_distance = np.mean(episode_distances)
            mean_stability = np.mean(episode_stabilities)
            mean_reward = np.mean(episode_reward)
            
            self.logger.record("eval/mean_distance", mean_distance)
            self.logger.record("eval/mean_stability", mean_stability)
            self.logger.record("eval/mean_reward", mean_reward)

            if self.verbose > 0:
                print(f"--- Timestep {self.num_timesteps}: Custom Eval ---")
                print(f"Avg Distance: {mean_distance:.2f} m, Avg Stability: {mean_stability:.4f}, Avg Reward: {mean_reward:.2f}")

            # ìµœê³  ì´ë™ ê±°ë¦¬ ëª¨ë¸ ì €ì¥
            if mean_distance > self.best_mean_distance:
                self.best_mean_distance = mean_distance
                self.model.save(os.path.join(self.save_path, "sac_walker2d_best_distance.zip"))
                if self.verbose > 0: print(f"  >> New best distance model saved: {mean_distance:.2f} m")

            # ìµœê³  ì•ˆì •ì„± ëª¨ë¸ ì €ì¥
            if mean_stability < self.best_mean_stability:
                self.best_mean_stability = mean_stability
                self.model.save(os.path.join(self.save_path, "sac_walker2d_best_stability.zip"))
                if self.verbose > 0: print(f"  >> New best stability model saved: {mean_stability:.4f}")
            
            # ìµœê³  ë³´ìƒ ëª¨ë¸ ì €ì¥
            if mean_reward > self.best_reward:
                self.best_reward = mean_reward
                self.model.save(os.path.join(self.save_path, "sac_walker2d_best_reward.zip"))
                if self.verbose > 0: print(f"  >> New best reward model saved: {mean_reward:.2f}")
            
            print("---------------------------------")
        
        return True


## --- ëª¨ë¸ í…ŒìŠ¤íŠ¸ì™€ ì˜ìƒ ë…¹í™” ---
def test_model(xml, model_path, seed, video_folder):
    print(f"--- '{model_path}' ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì‹œë“œ: {seed}) ---")
    
    # í™˜ê²½ ìƒì„±
    custom_xml_path = xml
    env = gym.make("Walker2d-v5", xml_file=custom_xml_path)
    env = PhasedGaitWrapper(env=env)
    
    # ë¹„ë””ì˜¤ ë…¹í™” ë˜í¼ ì ìš©
    os.makedirs(video_folder, exist_ok=True)
    model_name = os.path.splitext(os.path.basename(model_path))[0]
    video_prefix = f"{model_name}"
    env = RecordVideo(env, video_folder=video_folder, name_prefix=video_prefix, fps=30)
    
    # í›ˆë ¨ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    set_random_seed(seed)
    model = SAC.load(model_path, env=env)
    
    # í‰ê°€ ì‹œì‘
    obs, info = env.reset(seed=seed)
    
    # í‰ê°€ ì§€í‘œ ì´ˆê¸°í™”
    torso_angles = []
    total_reward = 0.0
    final_distance = 0.0
    done = False

    while not done:
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
        
        # í‰ê°€ ì§€í‘œ ìˆ˜ì§‘
        torso_angle = obs[1]
        torso_angles.append(torso_angle)
        total_reward += float(reward)

        if done:
            final_distance = info.get('x_position', 0)
    # ìµœì¢… ê²°ê³¼ ê³„ì‚° ë° ì¶œë ¥
    stability_score = np.std(torso_angles)

    utils.print_log("\n--- ìµœì¢… í‰ê°€ ê²°ê³¼ ---", model_path)
    utils.print_log(f"ëª¨ë¸: {model_path}", model_path)
    utils.print_log(f"ìµœì¢… ì´ë™ ê±°ë¦¬: {final_distance:.2f} m", model_path)
    utils.print_log(f"ì´ ë³´ìƒ: {total_reward:.2f}", model_path)
    utils.print_log(f"ëª¸í†µ í”ë“¤ë¦¼ (ì•ˆì •ì„±): {stability_score:.4f} (ë‚®ì„ìˆ˜ë¡ ì•ˆì •ì )", model_path)
    utils.print_log(f"ì˜ìƒ ì €ì¥ ìœ„ì¹˜: {video_folder}{video_prefix}.mp4", model_path)
    utils.print_log("-" * 30 + "\n", model_path)

    env.close()

# --- ë©”ì¸ í›ˆë ¨ ì½”ë“œ ---
if __name__ == "__main__":
    FOLDER_NAME = "custom_SAC_" + datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    SAVE_PATH = FOLDER_NAME + f"/results/"
    TENSORBOARD_PATH = FOLDER_NAME + "/tensorboard/"
    VIDEO_PATH = FOLDER_NAME + "/videos/"
    TOTAL_TIMESTEPS = 3000000
    
    # xml íŒŒì¼ ê²½ë¡œ ì„¤ì •
    current_file_path = os.path.abspath(__file__)
    current_dir = os.path.dirname(current_file_path)
    custom_xml_path = os.path.join(current_dir, 'xml/walker2d_hurdle.xml')

    # ì‹œë“œ ì„¤ì •
    SEED = 42
    utils.set_seed(SEED)

    # í›ˆë ¨ìš© í™˜ê²½
    train_env = gym.make("Walker2d-v5", xml_file=custom_xml_path)
    train_env = TimeLimit(train_env, max_episode_steps=1000)
    train_env = Monitor(train_env, SAVE_PATH)
    train_env = PhasedGaitWrapper(env=train_env)
    train_env.reset(seed=SEED) # í™˜ê²½ ì´ˆê¸°í™” ì‹œ ì‹œë“œ ì„¤ì •
    train_env.action_space.seed(SEED)

    # í‰ê°€ìš© í™˜ê²½
    eval_env = gym.make("Walker2d-v5", xml_file=custom_xml_path)
    eval_env = TimeLimit(eval_env, max_episode_steps=1000)
    eval_env = PhasedGaitWrapper(env=eval_env)
    eval_env.reset(seed=SEED) # í™˜ê²½ ì´ˆê¸°í™” ì‹œ ì‹œë“œ ì„¤ì •
    eval_env.action_space.seed(SEED)

    # cpu ì‚¬ìš©
    device = "cpu"
    print(f"Using device: {device}")

    # ì½œë°± ì„¤ì •
    callback = AdvancedEvalCallback(
        eval_env, 
        save_path=SAVE_PATH, 
        eval_freq=20000, 
        n_eval_episodes=5, 
        verbose=1)

    # ëª¨ë¸ ìƒì„±í•˜ê¸°
    model = SAC(
        "MlpPolicy", 
        train_env, 
        verbose=1, 
        seed=SEED, 
        device=device,
        tensorboard_log=TENSORBOARD_PATH 
    )

    # ëª¨ë¸ í•™ìŠµì‹œí‚¤ê¸°
    model.learn(
        total_timesteps=TOTAL_TIMESTEPS,
        callback=callback,
        tb_log_name="sac_walker2d"
    )

    # ìµœì¢… ëª¨ë¸ ì €ì¥í•˜ê¸°
    model.save(f"{SAVE_PATH}sac_walker2d_final.zip")
    print("ìµœì¢… ëª¨ë¸ ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    train_env.close()
    eval_env.close()

    # í…ŒìŠ¤íŠ¸
    test_model(
        xml=custom_xml_path,
        model_path=SAVE_PATH + "sac_walker2d_best_distance",
        seed=SEED,
        video_folder=VIDEO_PATH
    )
    test_model(
        xml=custom_xml_path,
        model_path=SAVE_PATH + "sac_walker2d_best_stability",
        seed=SEED,
        video_folder=VIDEO_PATH
    )
    test_model(
        xml=custom_xml_path,
        model_path=SAVE_PATH + "sac_walker2d_best_reward",
        seed=SEED,
        video_folder=VIDEO_PATH
    )
    test_model(
        xml=custom_xml_path,
        model_path=SAVE_PATH + "sac_walker2d_final",
        seed=SEED,
        video_folder=VIDEO_PATH

    )
