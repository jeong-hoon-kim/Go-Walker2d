import gymnasium as gym
from gymnasium import Wrapper
from stable_baselines3 import PPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.utils import set_random_seed
from stable_baselines3.common.callbacks import BaseCallback
import numpy as np
import os
import torch

# --- 1. ì»¤ìŠ¤í…€ ë³´ìƒ ë˜í¼ í´ë˜ìŠ¤ ---

class PacedWalkingWrapper(Wrapper):
    def __init__(self, env):
        super().__init__(env)
        
        # --- ğŸ† ì†ë„ ì œì–´ í•˜ì´í¼íŒŒë¼ë¯¸í„° ---
        self.target_velocity = 1.75  # ëª©í‘œ ê±·ê¸° ì†ë„ (m/s)
        self.velocity_tolerance = 0.5 # ì†ë„ í—ˆìš© ì˜¤ì°¨ (ì´ ê°’ì´ ì‘ì„ìˆ˜ë¡ ì—„ê²©í•´ì§)
        self.velocity_reward_weight = 2 # ì†ë„ ë³´ìƒì˜ ìµœëŒ€ í¬ê¸° (ìµœëŒ€ ë³´ë„ˆìŠ¤ ì ìˆ˜)
        
        # --- ì•ˆì •ì„± í˜ë„í‹° ê°€ì¤‘ì¹˜ ---
        self.stability_weight = 0.3
        self.flight_penalty_weight = 1
        
        # MuJoCo ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ë°œê³¼ ë°”ë‹¥ì˜ IDë¥¼ ë¯¸ë¦¬ ì°¾ì•„ë‘¡ë‹ˆë‹¤.
        self.left_foot_geom_id = self.env.unwrapped.model.geom('foot_left_geom').id
        self.right_foot_geom_id = self.env.unwrapped.model.geom('foot_geom').id
        self.floor_geom_id = self.env.unwrapped.model.geom('floor').id
        
    def _check_foot_contact(self):
        """ë‘ ë°œì´ ë°”ë‹¥ì— ë‹¿ì•„ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
        left_contact = False
        right_contact = False
        
        for contact in self.env.unwrapped.data.contact:
            geom_pair = {contact.geom1, contact.geom2}
            
            if self.left_foot_geom_id in geom_pair and self.floor_geom_id in geom_pair:
                left_contact = True
            if self.right_foot_geom_id in geom_pair and self.floor_geom_id in geom_pair:
                right_contact = True
        
        return left_contact, right_contact

    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        return obs, info

    def step(self, action):
        obs, original_reward, terminated, truncated, info = self.env.step(action)

        # 1. ê¸°ë³¸ ë³´ìƒì—ì„œ 'ìƒì¡´ ë³´ë„ˆìŠ¤'ì™€ 'ì»¨íŠ¸ë¡¤ ë¹„ìš©'ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        # ê¸°ì¡´ì˜ 'ì „ì§„ ë³´ìƒ'ì€ ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        healthy_reward = info.get('reward_survive', 1.0)
        ctrl_cost = info.get('reward_ctrl', 0)

        # 2. ëª¸í†µ ì•ˆì •ì„± í˜ë„í‹° (ìœ ì§€)
        stability_penalty = self.stability_weight * (np.abs(obs[1]) + 0.1 * np.abs(obs[10]))
        
        # --- ğŸ† 3. 'ì†ë„ ìƒí•œì„ ' ë³´ë„ˆìŠ¤ ê³„ì‚° ---
        
        # í˜„ì¬ ì „ì§„ ì†ë„ë¥¼ obs ë²¡í„°ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        current_velocity = obs[8]
        
        # ê°€ìš°ì‹œì•ˆ í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ë³´ìƒ ê³„ì‚°:
        # í˜„ì¬ ì†ë„ê°€ target_velocityì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë³´ìƒì´ velocity_reward_weightì— ê°€ê¹Œì›Œì§€ê³ ,
        # ë©€ì–´ì§ˆìˆ˜ë¡ 0ì— ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤.
        velocity_bonus = self.velocity_reward_weight * \
                         np.exp(-np.square(current_velocity - self.target_velocity) / (2 * np.square(self.velocity_tolerance)))
                         
        # --- ğŸ† 'ê³µì¤‘ ì²´ê³µ' í˜ë„í‹° ê³„ì‚° ---
        left_foot_on_ground, right_foot_on_ground = self._check_foot_contact()
        flight_penalty = 0
        if not left_foot_on_ground and not right_foot_on_ground:
            flight_penalty = self.flight_penalty_weight

        # 4. ëª¨ë“  ìš”ì†Œë¥¼ í•©ì‚°í•˜ì—¬ ìµœì¢… ë³´ìƒ ê³„ì‚°
        new_reward = (
            velocity_bonus
            + healthy_reward 
            + ctrl_cost
            - stability_penalty
            - flight_penalty
        )
        
        return obs, new_reward, terminated, truncated, info

# --- 2. ì»¤ìŠ¤í…€ í‰ê°€ ì½œë°± í´ë˜ìŠ¤ ì •ì˜ ---
class AdvancedEvalCallback(BaseCallback):
    def __init__(self, eval_env, save_path, eval_freq, n_eval_episodes, verbose):
        super(AdvancedEvalCallback, self).__init__(verbose)
        self.eval_env = eval_env
        self.save_path = save_path
        self.eval_freq = eval_freq
        self.n_eval_episodes = n_eval_episodes
        
        # ê° ì§€í‘œë³„ ìµœê³  ê¸°ë¡ì„ ì €ì¥í•  ë³€ìˆ˜
        self.best_mean_distance = -np.inf
        self.best_mean_stability = np.inf
        self.best_reward = -np.inf

    def _on_step(self) -> bool:
        if self.n_calls % self.eval_freq == 0:
            episode_distances, episode_stabilities = [], []
            for _ in range(self.n_eval_episodes):
                obs, info = self.eval_env.reset()
                done = False
                torso_angles = []
                final_distance = 0
                while not done:
                    action, _ = self.model.predict(obs, deterministic=True)
                    obs, reward, terminated, truncated, info = self.eval_env.step(action)
                    done = terminated or truncated
                    
                    # Walker2d-v5ì˜ obs[1]ëŠ” ëª¸í†µ ê°ë„(torso angle)ì…ë‹ˆë‹¤. (v3, v4ì™€ ë‹¤ë¦„)
                    torso_angles.append(obs[1]) 
                    if done: 
                        final_distance = info.get('x_position', 0)
                
                episode_distances.append(final_distance)
                episode_stabilities.append(np.std(torso_angles))

            mean_distance = np.mean(episode_distances)
            mean_stability = np.mean(episode_stabilities)
            
            self.logger.record("eval/mean_distance", mean_distance)
            self.logger.record("eval/mean_stability", mean_stability)

            if self.verbose > 0:
                print(f"--- Timestep {self.num_timesteps}: Custom Eval ---")
                print(f"Avg Distance: {mean_distance:.2f} m, Avg Stability: {mean_stability:.4f}")
            # ìµœê³  ì´ë™ ê±°ë¦¬ ëª¨ë¸ ì €ì¥
            if mean_distance > self.best_mean_distance:
                self.best_mean_distance = mean_distance
                self.model.save(os.path.join(self.save_path, "ppo_walker2d_best_distance.zip"))
                if self.verbose > 0: print(f"  >> New best distance model saved: {mean_distance:.2f} m")

            # ìµœê³  ì•ˆì •ì„± ëª¨ë¸ ì €ì¥
            if mean_stability < self.best_mean_stability:
                self.best_mean_stability = mean_stability
                self.model.save(os.path.join(self.save_path, "ppo_walker2d_best_stability.zip"))
                if self.verbose > 0: print(f"  >> New best stability model saved: {mean_stability:.4f}")
            
            print("---------------------------------")
        
        return True

# --- 3. ë©”ì¸ í›ˆë ¨ ì½”ë“œ ---
if __name__ == "__main__":
    # --- ì„¤ì • ---
    MODEL_NAME = "ppo_walker2d_custom_reward_v4"
    SAVE_PATH = f"results/{MODEL_NAME}/"
    LOG_PATH = "tensorboard_logs/"
    TOTAL_TIMESTEPS = 3000000
    SEED = 42

    set_random_seed(SEED)
    os.makedirs(SAVE_PATH, exist_ok=True)
    os.makedirs(LOG_PATH, exist_ok=True)

    # í›ˆë ¨ìš© í™˜ê²½
    train_env = gym.make("Walker2d-v5")
    train_env = PacedWalkingWrapper(train_env)
    train_env = Monitor(train_env, SAVE_PATH)

    # í‰ê°€ìš© í™˜ê²½
    eval_env = gym.make("Walker2d-v5")

    # ì‚¬ìš© ê°€ëŠ¥í•œ ì¥ì¹˜ í™•ì¸ (GPU ìš°ì„ )
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    # ì½œë°± ì„¤ì •
    callback = AdvancedEvalCallback(
    eval_env=eval_env, 
    save_path=SAVE_PATH,
    eval_freq=20000,
    n_eval_episodes=5,
    verbose=1
)

    # ëª¨ë¸ ìƒì„±í•˜ê¸°
    model = PPO(
        "MlpPolicy", 
        train_env, 
        verbose=1, 
        seed=SEED, 
        device=device,
        tensorboard_log=LOG_PATH 
    )

    # ëª¨ë¸ í•™ìŠµì‹œí‚¤ê¸°
    model.learn(
        total_timesteps=TOTAL_TIMESTEPS,
        callback=callback,
        tb_log_name=MODEL_NAME
    )

    # ìµœì¢… ëª¨ë¸ ì €ì¥í•˜ê¸°
    model.save(f"{SAVE_PATH}{MODEL_NAME}_final.zip")
    print("ìµœì¢… ëª¨ë¸ ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    train_env.close()
    eval_env.close()